{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_track = pd.read_csv('data/track.csv')\n",
    "df_album = pd.read_csv('data/album.csv')\n",
    "df_artist = pd.read_csv('data/artist.csv')\n",
    "df_rating = pd.read_csv('data/rating.csv')\n",
    "df_similar = pd.read_csv('data/track_similarity.csv')\n",
    "df_weekly_rating = pd.read_csv('data/weekly_rating.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= Convert Names to IDs =============================\n",
    "\n",
    "def convert_artist_name_to_id(df_artist, artist_name):\n",
    "    return df_artist[df_artist['name'] == artist_name]['artist_id'].values[0]\n",
    "\n",
    "def convert_album_name_to_id(album_name):\n",
    "    return df_album[df_album['name'] == album_name]['album_id'].values[0]\n",
    "\n",
    "def convert_track_name_to_id(track_name):\n",
    "    return df_track[df_track['name'] == track_name]['track_id'].values[0]\n",
    "\n",
    "# ======================= Convert Ids to Names =============================\n",
    "\n",
    "def convert_track_id_to_name(track_id):\n",
    "    return df_track[df_track['track_id'] == track_id]['name'].values[0]\n",
    "\n",
    "def convert_album_id_to_name(album_id):\n",
    "    return df_album[df_album['album_id'] == album_id]['name'].values[0]\n",
    "\n",
    "def convert_artist_id_to_name(artist_id):\n",
    "    return df_artist[df_artist['artist_id'] == artist_id]['name'].values[0]\n",
    "\n",
    "# ======================= Others =============================\n",
    "\n",
    "def get_all_artist_albums(artist_name, byName=False):\n",
    "    all_albums = df_artist[df_artist['name'] == artist_name]['all_albums'].values[0]\n",
    "    all_albums = all_albums[1:-1].split(',')\n",
    "    all_albums = [int(album) for album in all_albums]\n",
    "\n",
    "    if byName:\n",
    "        all_albums = [convert_album_id_to_name(album) for album in all_albums]\n",
    "\n",
    "    return all_albums\n",
    "\n",
    "def get_all_artist_tracks(artist_name, byName=False):\n",
    "    all_tracks = df_artist[df_artist['name'] == artist_name]['all_tracks'].values[0]\n",
    "    all_tracks = all_tracks[1:-1].split(',')\n",
    "    all_tracks = [int(track) for track in all_tracks]\n",
    "\n",
    "    all_tracks = [convert_track_id_to_name(track) for track in all_tracks]\n",
    "\n",
    "    return all_tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of different artists\n",
    "artist_count = len(df_artist)\n",
    "print(\"Number of different artists:\", artist_count) \n",
    "\n",
    "# number of different albums\n",
    "album_count = len(df_album)\n",
    "print(\"Number of different albums:\", album_count)\n",
    "\n",
    "# number of different tracks\n",
    "track_count = len(df_track)\n",
    "print(\"Number of different tracks:\", track_count)\n",
    "\n",
    "# mean number of tracks per album\n",
    "mean_tracks_per_album = round(track_count / album_count, 1)\n",
    "print(\"\\nMean number of tracks per album:\", mean_tracks_per_album)\n",
    "\n",
    "# mean number of albums per artist\n",
    "mean_albums_per_artist = round(album_count / artist_count, 1)   \n",
    "print(\"Mean number of albums per artist:\", mean_albums_per_artist)\n",
    "\n",
    "# mean number of tracks per artist\n",
    "mean_tracks_per_artist = round(track_count / artist_count, 1)\n",
    "print(\"Mean number of tracks per artist:\", mean_tracks_per_artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows containing 'bieber', handling NaN values\n",
    "bieber_artists = df_artist[df_artist['name'].str.contains('bieber', na=False, case=False)]\n",
    "\n",
    "# Display the results\n",
    "print('Artists containing \"Bieber\":\\n', bieber_artists)\n",
    "print('Number of artists containing \"Bieber\":', len(bieber_artists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the artist have either \"&\" or \"and\" or \"featuring\" in their name\n",
    "\n",
    "# Filter rows containing '&', 'and' or 'featuring', handling NaN values\n",
    "and_artists = df_artist[df_artist['name'].str.contains('&|featuring', na=False)]\n",
    "\n",
    "# Display the results\n",
    "\n",
    "print('Artists containing \"&\", \"and\" or \"featuring\":\\n', and_artists)\n",
    "print('Number of artists containing \"&\", \"and\" or \"featuring\":', len(and_artists))\n",
    "\n",
    "# number of different track_ids in the track_similarity table because there are duplicates\n",
    "\n",
    "# Get the number of unique track_ids in the track_similarity table\n",
    "unique_track_ids = df_similar[['track_id_1', 'track_id_2']].stack().unique()\n",
    "print('Number of unique track_ids in the track_similarity table:', len(unique_track_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracks & Albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all tracks of an artist\n",
    "artist_name = 'Taylor Swift'\n",
    "\n",
    "# get all albums of an artist\n",
    "all_albums = get_all_artist_albums(artist_name, byName=True)\n",
    "all_tracks = get_all_artist_tracks(artist_name, byName=True)\n",
    "\n",
    "print(f\"\\nAlbums by {artist_name}:\")\n",
    "for album in all_albums:\n",
    "    print(album)\n",
    "\n",
    "print(f\"\\n====================\\nTracks by {artist_name}:\")\n",
    "for track in all_tracks:\n",
    "    print(track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the 20 artists with the most tracks\n",
    "artist_track_count = df_track.groupby('artist_id').size().reset_index(name='track_count')\n",
    "artist_track_count = artist_track_count.sort_values(by='track_count', ascending=False)\n",
    "\n",
    "# make a distribution of the number of tracks per artist\n",
    "plt.hist(artist_track_count['track_count'], bins=50)\n",
    "plt.xlabel('Number of tracks')\n",
    "plt.ylabel('Number of artists')\n",
    "plt.yscale('log')\n",
    "plt.title('Distribution of the number of tracks per artist')\n",
    "\n",
    "artist_track_count = artist_track_count.head(20)\n",
    "artist_track_count['artist_name'] = artist_track_count['artist_id'].apply(convert_artist_id_to_name)\n",
    "print(\"Top 20 artists by number of tracks:\\n\" + str(artist_track_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tracks per album\n",
    "album_track_count = df_track.groupby('album_id').size().reset_index(name='track_count')\n",
    "album_track_count = album_track_count.sort_values(by='track_count', ascending=False)\n",
    "\n",
    "# make a distribution of the number of tracks per album\n",
    "plt.hist(album_track_count['track_count'], bins=50)\n",
    "plt.xlabel('Number of tracks')\n",
    "plt.ylabel('Number of albums')\n",
    "plt.yscale('log')\n",
    "plt.title('Distribution of the number of tracks per album')\n",
    "\n",
    "album_track_count = album_track_count.head(20)\n",
    "album_track_count['album_name'] = album_track_count['album_id'].apply(convert_album_id_to_name)\n",
    "print(\"Top 20 albums by number of tracks:\\n\" + str(album_track_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_track_lists_sim(track_id_list_1, track_id_list_2):\n",
    "\n",
    "    ret_list = []\n",
    "\n",
    "    for main_track_id in track_id_list_1:\n",
    "        for other_track_id in track_id_list_2:    \n",
    "            \n",
    "            similarity = df_similar[(df_similar['track_id_1'] == main_track_id) & (df_similar['track_id_2'] == other_track_id)]\n",
    "            \n",
    "            if not similarity.empty:\n",
    "                name1 = convert_track_id_to_name(main_track_id)\n",
    "                name2 = convert_track_id_to_name(other_track_id)\n",
    "                ret_list.append((name1, name2, float(similarity.iloc[0][\"sim_degree\"])))\n",
    "\n",
    "            similarity = df_similar[(df_similar['track_id_2'] == main_track_id) & (df_similar['track_id_1'] == other_track_id)]\n",
    "            \n",
    "            if not similarity.empty:\n",
    "                name1 = convert_track_id_to_name(main_track_id)\n",
    "                name2 = convert_track_id_to_name(other_track_id)\n",
    "                ret_list.append((name1, name2, float(similarity.iloc[0][\"sim_degree\"])))\n",
    "\n",
    "    ret = sorted(ret_list, key=lambda x: x[2], reverse=True)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all artist_id 1 tracks with all artist_id 2 tracks using track_similarity\n",
    "\n",
    "artist_1 = 'Justin Bieber'\n",
    "artist_2 = 'Calvin Harris'\n",
    "\n",
    "artist_1_tracks_ids = get_all_artist_tracks(artist_1)\n",
    "artist_2_tracks_ids = get_all_artist_tracks(artist_2)\n",
    "\n",
    "print(f'Tracks by {artist_1}: {len(artist_1_tracks_ids)}')\n",
    "print(f'Tracks by {artist_2}: {len(artist_2_tracks_ids)}\\n')\n",
    "\n",
    "# Get the similarity between all tracks by artist 1 and all tracks by the artist 2\n",
    "result = compare_track_lists_sim(artist_1_tracks_ids, artist_2_tracks_ids) \n",
    "for r in result:\n",
    "    print(r)\n",
    "\n",
    "if len(result) == 0:\n",
    "    print('No similarities found between the tracks of the two artists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all track ids that do not have a similarity value\n",
    "temp_list = []\n",
    "track_ids = df_track['track_id'].to_list()\n",
    "for track_id in track_ids:\n",
    "    if df_similar[(df_similar['track_id_1'] == track_id) | (df_similar['track_id_2'] == track_id)].empty:\n",
    "        temp_list.append(track_id)\n",
    "\n",
    "print('Track ids without similarity value:', f\"{temp_list[:10]} ...\" if len(temp_list) > 10 else temp_list)\n",
    "print('Number of track ids without similarity value:', len(temp_list))\n",
    "print('Percentage of track ids without similarity value:', round(len(temp_list) / track_count * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print df_similar\n",
    "print(df_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import community\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "df_similar_small = df_similar.head(500)\n",
    "\n",
    "# Add edges with weights\n",
    "for _, row in df_similar_small.iterrows():\n",
    "    G.add_edge(row['track_id_1'], row['track_id_2'], weight=row['sim_degree'])\n",
    "\n",
    "# Set up figure size and resolution BEFORE drawing anything\n",
    "plt.figure(figsize=(40, 40), dpi=500)\n",
    "\n",
    "# Layout for the graph\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Find communities\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "\n",
    "# Assign colors to each community\n",
    "node_colors = {}\n",
    "for idx, comm in enumerate(communities):\n",
    "    for node in comm:\n",
    "        node_colors[node] = idx\n",
    "\n",
    "# Draw nodes with community colors\n",
    "nx.draw_networkx_nodes(\n",
    "    G, pos,\n",
    "    node_size=500,\n",
    "    node_color=[node_colors[node] for node in G.nodes()]\n",
    ")\n",
    "\n",
    "# Draw edges with thickness based on weight\n",
    "nx.draw_networkx_edges(\n",
    "    G,\n",
    "    pos,\n",
    "    edgelist=G.edges(data=True),\n",
    "    width=[edge[2]['weight'] * 5 for edge in G.edges(data=True)]  # Scale the thickness\n",
    ")\n",
    "\n",
    "# Draw labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=5, font_color=\"black\")\n",
    "\n",
    "# Display the graph\n",
    "plt.title(\"Song Similarity Graph\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe where, instead of df_similar track_id, we have the authors names\n",
    "df_similar_artists = df_similar.copy()\n",
    "df_similar_artists['artist_id_1'] = df_similar_artists['track_id_1'].apply(lambda x: df_track[df_track['track_id'] == x]['artist_id'].values[0])\n",
    "df_similar_artists['artist_id_2'] = df_similar_artists['track_id_2'].apply(lambda x: df_track[df_track['track_id'] == x]['artist_id'].values[0])\n",
    "df_similar_artists['artist_name_1'] = df_similar_artists['artist_id_1'].apply(lambda x: df_artist[df_artist['artist_id'] == x]['name'].values[0])\n",
    "df_similar_artists['artist_name_2'] = df_similar_artists['artist_id_2'].apply(lambda x: df_artist[df_artist['artist_id'] == x]['name'].values[0])\n",
    "\n",
    "# print the new dataframe\n",
    "print(df_similar_artists)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep the sim_degree, artist_name_1 and artist_name_2 columns\n",
    "\n",
    "df_similar_artists = df_similar_artists[['sim_degree', 'artist_name_1', 'artist_name_2']]\n",
    "\n",
    "# move sim_degree to the end\n",
    "df_similar_artists = df_similar_artists[['artist_name_1', 'artist_name_2', 'sim_degree']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similar_artists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove df_similar_artists entries where artist_name_1 == artist_name_2\n",
    "df_similar_artists = df_similar_artists[df_similar_artists['artist_name_1'] != df_similar_artists['artist_name_2']]\n",
    "df_similar_artists.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import community\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Use artist dataframe\n",
    "df_similar_artists_small = df_similar_artists.head(100)  # Adjust for your actual dataframe\n",
    "\n",
    "# only keep rows with sim_degree == 1\n",
    "df_similar_artists_1 = df_similar_artists[df_similar_artists['sim_degree'] == 1]\n",
    "\n",
    "# Add edges with weights\n",
    "for _, row in df_similar_artists.iterrows():\n",
    "    G.add_edge(row['artist_name_1'], row['artist_name_2'], weight=row['sim_degree'])\n",
    "\n",
    "# Set up figure size and resolution BEFORE drawing anything\n",
    "plt.figure(figsize=(20, 20), dpi=300)\n",
    "\n",
    "# Layout for the graph\n",
    "pos = nx.spring_layout(G, k=0.7, seed=42)  # Adjust k for spacing\n",
    "\n",
    "# Find communities\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "\n",
    "# Assign colors to each community\n",
    "node_colors = {}\n",
    "for idx, comm in enumerate(communities):\n",
    "    for node in comm:\n",
    "        node_colors[node] = idx\n",
    "\n",
    "# draw node size based on degree\n",
    "node_size = [G.degree(node) * 100 for node in G.nodes()]\n",
    "\n",
    "# Draw nodes with community colors\n",
    "nx.draw_networkx_nodes(\n",
    "    G, pos,\n",
    "    node_size=node_size,\n",
    "    node_color=[node_colors[node] for node in G.nodes()]\n",
    ")\n",
    "\n",
    "\n",
    "# Draw edges with thickness based on weight\n",
    "nx.draw_networkx_edges(\n",
    "    G,\n",
    "    pos,\n",
    "    edgelist=G.edges(data=True),\n",
    "    width=[edge[2]['weight'] * 2.5 for edge in G.edges(data=True)]  # Scale the thickness\n",
    ")\n",
    "\n",
    "# Draw labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=8, font_color=\"gray\")\n",
    "\n",
    "# Display the graph\n",
    "plt.title(\"Artist Similarity Graph\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe of the artist name to number of entries in the df_similar_artists dataframe\n",
    "artist_count = df_similar_artists['artist_name_1'].value_counts().reset_index()\n",
    "artist_count.columns = ['artist_name', 'count']\n",
    "artist_count.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Year Analysis\n",
    "In order to be able to consider data from all three sources, we will analyze the interval between 01/05/2013 and 01/05/2014. For epoch analysis between 29/04/2013 and 04/05/2014, which means weeks 487 to 539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the year dataframe, which is from weekly_rating, get all rows that have time_epoch >= 487 and time_epoch <= 539\n",
    "df_year = df_weekly_rating[(df_weekly_rating['time_epoch'] >= 487) & (df_weekly_rating['time_epoch'] <= 539)]\n",
    "df_year.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the yearly dataframe, lets analyze first a single track, id 428\n",
    "df_year_single = df_year[df_year['track_id'] == 428]\n",
    "df_year_single.shape\n",
    "\n",
    "# Plot the track rating over the year, need three lines, one for position_billboard, other for position_spotify and other for position_lastfm, invert the y axis to show the rating from 1 to 100\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_year_single['time_epoch'], df_year_single['position_billboard'], label='Billboard')\n",
    "plt.plot(df_year_single['time_epoch'], df_year_single['position_spotify'], label='Spotify')\n",
    "plt.plot(df_year_single['time_epoch'], df_year_single['position_lastfm'], label='LastFM')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Track 428')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect which track has the highest correlation between the three positions, using spearman correlation, and only using rows where both positions are not NaN\n",
    "# Get the track ids\n",
    "track_ids = df_year['track_id'].unique()\n",
    "\n",
    "# Create a list to store the correlations\n",
    "correlations = []\n",
    "\n",
    "# Iterate over the track ids\n",
    "for track_id in track_ids:\n",
    "    # Get the track data\n",
    "    df_year_single = df_year[df_year['track_id'] == track_id]\n",
    "    billboard_spotify_filtered = df_year_single.dropna(subset=['position_billboard','position_spotify'],how='any')\n",
    "    billboard_lastfm_filtered = df_year_single.dropna(subset=['position_billboard','position_lastfm'],how='any')\n",
    "    spotify_lastfm_filtered = df_year_single.dropna(subset=['position_spotify','position_lastfm'],how='any')\n",
    "    # Calculate the correlation between the positions, but use only the rows where both positions are not NaN\n",
    "    corr_billboard_spotify, _ = spearmanr(billboard_spotify_filtered['position_billboard'], billboard_spotify_filtered['position_spotify'])\n",
    "    corr_billboard_lastfm, _ = spearmanr(billboard_lastfm_filtered['position_billboard'], billboard_lastfm_filtered['position_lastfm'])\n",
    "    corr_spotify_lastfm, _ = spearmanr(spotify_lastfm_filtered['position_spotify'], spotify_lastfm_filtered['position_lastfm'])\n",
    "    # Save the correlations\n",
    "    correlations.append((track_id, corr_billboard_spotify, corr_billboard_lastfm, corr_spotify_lastfm))\n",
    "\n",
    "# Replace NaN values in each correlation with 0\n",
    "correlations = [(track_id, corr_billboard_spotify if not np.isnan(corr_billboard_spotify) else 0, corr_billboard_lastfm if not np.isnan(corr_billboard_lastfm) else 0, corr_spotify_lastfm if not np.isnan(corr_spotify_lastfm) else 0) for track_id, corr_billboard_spotify, corr_billboard_lastfm, corr_spotify_lastfm in correlations]\n",
    "\n",
    "# Sort the correlations for billboard and spotify\n",
    "correlations = sorted(correlations, key=lambda x: x[1], reverse=True)\n",
    "# Get the track with the highest correlation\n",
    "best_track_id, corr_billboard_spotify, corr_billboard_lastfm, corr_spotify_lastfm = correlations[0]\n",
    "# Print the results\n",
    "print(f'Track with highest correlation between Billboard and Spotify: {convert_track_id_to_name(best_track_id), corr_billboard_spotify}')\n",
    "\n",
    "# Sort the correlations for billboard and lastfm\n",
    "correlations = sorted(correlations, key=lambda x: x[2], reverse=True)\n",
    "# Get the track with the highest correlation\n",
    "best_track_id, corr_billboard_spotify, corr_billboard_lastfm, corr_spotify_lastfm = correlations[0]\n",
    "# Print the results\n",
    "print(f'Track with highest correlation between Billboard and LastFM: {convert_track_id_to_name(best_track_id), corr_billboard_lastfm}')\n",
    "\n",
    "# Sort the correlations for spotify and lastfm\n",
    "correlations = sorted(correlations, key=lambda x: x[3], reverse=True)\n",
    "# Get the track with the highest correlation\n",
    "best_track_id, corr_billboard_spotify, corr_billboard_lastfm, corr_spotify_lastfm = correlations[0]\n",
    "# Print the results\n",
    "print(f'Track with highest correlation between Spotify and LastFM: {convert_track_id_to_name(best_track_id), corr_spotify_lastfm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In yearly data, identify which track_ids have 53 entries\n",
    "track_id = df_year['track_id'].value_counts()\n",
    "track_id = track_id[track_id == 53].index\n",
    "track_id = track_id.tolist()\n",
    "print('Track ids with 53 entries:', track_id)\n",
    "\n",
    "# Get the tracks names\n",
    "track_names = [convert_track_id_to_name(track) for track in track_id]\n",
    "print('Track names with 53 entries:', track_names)\n",
    "\n",
    "# Get the values on the columns position_billboard, position_spotify and position_lastfm for the track with 53 entries\n",
    "df_year_53 = df_year[df_year['track_id'].isin(track_id)]\n",
    "df_year_53 = df_year_53[['track_id','time_epoch' , 'position_billboard', 'position_spotify', 'position_lastfm']]\n",
    "\n",
    "# Plot the evolution of the three positions for the track with 53 entries\n",
    "plt.figure(figsize=(10, 5))\n",
    "for track in track_id:\n",
    "    df_year_single = df_year_53[df_year_53['track_id'] == track]\n",
    "    plt.plot(df_year_single['time_epoch'], df_year_single['position_billboard'], label='Billboard')\n",
    "    plt.plot(df_year_single['time_epoch'], df_year_single['position_spotify'], label='Spotify')\n",
    "    plt.plot(df_year_single['time_epoch'], df_year_single['position_lastfm'], label='LastFM')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Position')\n",
    "    plt.title(f'Track {convert_track_id_to_name(track)} - Id of {track}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Billboard Top 100 Consecutive Weeks Per Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the results\n",
    "billboard_top100_weeks = []\n",
    "\n",
    "# Iterate over the track ids\n",
    "for track_id in track_ids:\n",
    "    # Get the track data\n",
    "    df_year_single = df_year[df_year['track_id'] == track_id]\n",
    "    # Sort by time_epoch to ensure chronological order\n",
    "    df_year_single = df_year_single.sort_values('time_epoch')\n",
    "    # Filter out rows without a position in the Billboard top 100\n",
    "    df_year_single = df_year_single.dropna(subset=['position_billboard'])\n",
    "    \n",
    "    if df_year_single.empty:\n",
    "        continue\n",
    "\n",
    "    # Calculate the number of consecutive weeks\n",
    "    weeks = 0\n",
    "    max_consecutive_weeks = 0\n",
    "    last_epoch = None\n",
    "    starting_week_max = 0\n",
    "    ending_week_max = 0\n",
    "    starting_pos_max = 0\n",
    "    ending_pos_max = 0\n",
    "\n",
    "    for _, row in df_year_single.iterrows():\n",
    "        current_epoch = row['time_epoch']\n",
    "        if last_epoch is None or current_epoch - last_epoch == 1:\n",
    "            # If consecutive, increment the counter\n",
    "            weeks += 1\n",
    "        else:\n",
    "            # If not consecutive, reset the counter\n",
    "            weeks = 1\n",
    "        # Update the maximum consecutive weeks\n",
    "        if weeks > max_consecutive_weeks:\n",
    "            max_consecutive_weeks = weeks\n",
    "            starting_week_max = current_epoch - weeks + 1\n",
    "            ending_week_max = current_epoch\n",
    "            starting_pos_max = df_year_single[df_year_single['time_epoch'] == starting_week_max]['position_billboard'].values[0]\n",
    "            ending_pos_max = df_year_single[df_year_single['time_epoch'] == ending_week_max]['position_billboard'].values[0]\n",
    "        last_epoch = current_epoch\n",
    "\n",
    "    # Get the first and last positions of the track\n",
    "    first_position = df_year_single['position_billboard'].iloc[0]\n",
    "    last_position = df_year_single['position_billboard'].iloc[-1]\n",
    "\n",
    "    # Save the results\n",
    "    billboard_top100_weeks.append((track_id, first_position, last_position, max_consecutive_weeks, starting_week_max, ending_week_max, starting_pos_max, ending_pos_max))\n",
    "\n",
    "# Sort the results by the number of consecutive weeks\n",
    "billboard_top100_weeks = sorted(billboard_top100_weeks, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print('Tracks with the highest number of consecutive weeks in the Billboard top 100:')\n",
    "#for track_id, first_position, last_position, max_consecutive_weeks, starting_week_max, ending_week_max, starting_pos_max, ending_pos_max in billboard_top100_weeks:\n",
    "#    if max_consecutive_weeks > 1:\n",
    "#        print(f'Track: {convert_track_id_to_name(track_id)} of id {track_id} - First Position: {first_position} - Last Position: {last_position} - Max Consecutive Weeks: {max_consecutive_weeks} - Starting Week: {starting_week_max} - Ending Week: {ending_week_max} - Starting Position: {starting_pos_max} - Ending Position: {ending_pos_max}')\n",
    "\n",
    "print('Tracks that did not start on the first week:')\n",
    "for track_id, first_position, last_position, max_consecutive_weeks, starting_week_max, ending_week_max, starting_pos_max, ending_pos_max in billboard_top100_weeks:\n",
    "    if starting_week_max != 487 and max_consecutive_weeks > 1:\n",
    "        print(f'Track: {convert_track_id_to_name(track_id)} of id {track_id} - First Position: {first_position} - Last Position: {last_position} - Max Consecutive Weeks: {max_consecutive_weeks} - Starting Week: {starting_week_max} - Ending Week: {ending_week_max} - Starting Position: {starting_pos_max} - Ending Position: {ending_pos_max}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last.fm Top 100 Consecutive Weeks Per Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastfm_top100_weeks = []\n",
    "\n",
    "# Iterate over the track ids\n",
    "\n",
    "for track_id in track_ids:\n",
    "    # Get the track data\n",
    "    df_year_single = df_year[df_year['track_id'] == track_id]\n",
    "    # Sort by time_epoch to ensure chronological order\n",
    "    df_year_single = df_year_single.sort_values('time_epoch')\n",
    "    # Filter out rows without a position in the LastFM top 100\n",
    "    df_year_single = df_year_single.dropna(subset=['position_lastfm'])\n",
    "    \n",
    "    if df_year_single.empty:\n",
    "        continue\n",
    "\n",
    "    # Calculate the number of consecutive weeks\n",
    "    weeks = 0\n",
    "    max_consecutive_weeks = 0\n",
    "    last_epoch = None\n",
    "    starting_week_max = 0\n",
    "    ending_week_max = 0\n",
    "    starting_pos_max = 0\n",
    "    ending_pos_max = 0\n",
    "\n",
    "    for _, row in df_year_single.iterrows():\n",
    "        current_epoch = row['time_epoch']\n",
    "        if last_epoch is None or current_epoch - last_epoch == 1:\n",
    "            # If consecutive, increment the counter\n",
    "            weeks += 1\n",
    "        else:\n",
    "            # If not consecutive, reset the counter\n",
    "            weeks = 1\n",
    "        # Update the maximum consecutive weeks\n",
    "        if weeks > max_consecutive_weeks:\n",
    "            max_consecutive_weeks = weeks\n",
    "            starting_week_max = current_epoch - weeks + 1\n",
    "            ending_week_max = current_epoch\n",
    "            starting_pos_max = df_year_single[df_year_single['time_epoch'] == starting_week_max]['position_lastfm'].values[0]\n",
    "            ending_pos_max = df_year_single[df_year_single['time_epoch'] == ending_week_max]['position_lastfm'].values[0]\n",
    "        last_epoch = current_epoch\n",
    "\n",
    "    # Get the first and last positions of the track\n",
    "    first_position = df_year_single['position_lastfm'].iloc[0]\n",
    "    last_position = df_year_single['position_lastfm'].iloc[-1]\n",
    "\n",
    "    # Save the results\n",
    "    lastfm_top100_weeks.append((track_id, first_position, last_position, max_consecutive_weeks, starting_week_max, ending_week_max, starting_pos_max, ending_pos_max))\n",
    "\n",
    "# Sort the results by the number of consecutive weeks\n",
    "lastfm_top100_weeks = sorted(lastfm_top100_weeks, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print('Tracks with the highest number of consecutive weeks in the LastFM top 100:')\n",
    "#for track_id, first_position, last_position, max_consecutive_weeks, starting_week_max, ending_week_max, starting_pos_max, ending_pos_max in lastfm_top100_weeks:\n",
    "#    if max_consecutive_weeks > 1:\n",
    "#        print(f'Track: {convert_track_id_to_name(track_id)} of id {track_id} - First Position: {first_position} - Last Position: {last_position} - Max Consecutive Weeks: {max_consecutive_weeks} - Starting Week: {starting_week_max} - Ending Week: {ending_week_max} - Starting Position: {starting_pos_max} - Ending Position: {ending_pos_max}')\n",
    "\n",
    "print('Tracks that did not start on the first week:')\n",
    "\n",
    "for track_id, first_position, last_position, max_consecutive_weeks, starting_week_max, ending_week_max, starting_pos_max, ending_pos_max in lastfm_top100_weeks:\n",
    "    if starting_week_max != 487 and max_consecutive_weeks > 1:\n",
    "        print(f'Track: {convert_track_id_to_name(track_id)} of id {track_id} - First Position: {first_position} - Last Position: {last_position} - Max Consecutive Weeks: {max_consecutive_weeks} - Starting Week: {starting_week_max} - Ending Week: {ending_week_max} - Starting Position: {starting_pos_max} - Ending Position: {ending_pos_max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spotify Top 100 Consecutive Weeks Per Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_top100_weeks = []\n",
    "\n",
    "# Iterate over the track ids\n",
    "\n",
    "for track_id in track_ids:\n",
    "    # Get the track data\n",
    "    df_year_single = df_year[df_year['track_id'] == track_id]\n",
    "    # Sort by time_epoch to ensure chronological order\n",
    "    df_year_single = df_year_single.sort_values('time_epoch')\n",
    "    # Filter out rows without a position in the Spotify top 100\n",
    "    df_year_single = df_year_single.dropna(subset=['position_spotify'])\n",
    "    \n",
    "    if df_year_single.empty:\n",
    "        continue\n",
    "\n",
    "    # Calculate the number of consecutive weeks\n",
    "    weeks = 0\n",
    "    max_consecutive_weeks = 0\n",
    "    last_epoch = None\n",
    "    starting_week_max = 0\n",
    "    ending_week_max = 0\n",
    "    starting_pos_max = 0\n",
    "    ending_pos_max = 0\n",
    "\n",
    "    for _, row in df_year_single.iterrows():\n",
    "        current_epoch = row['time_epoch']\n",
    "        if last_epoch is None or current_epoch - last_epoch == 1:\n",
    "            # If consecutive, increment the counter\n",
    "            weeks += 1\n",
    "        else:\n",
    "            # If not consecutive, reset the counter\n",
    "            weeks = 1\n",
    "        # Update the maximum consecutive weeks\n",
    "        if weeks > max_consecutive_weeks:\n",
    "            max_consecutive_weeks = weeks\n",
    "            starting_week_max = current_epoch - weeks + 1\n",
    "            ending_week_max = current_epoch\n",
    "            starting_pos_max = df_year_single[df_year_single['time_epoch'] == starting_week_max]['position_spotify'].values[0]\n",
    "            ending_pos_max = df_year_single[df_year_single['time_epoch'] == ending_week_max]['position_spotify'].values[0]\n",
    "        last_epoch = current_epoch\n",
    "\n",
    "    # Get the first and last positions of the track\n",
    "    first_position = df_year_single['position_spotify'].iloc[0]\n",
    "    last_position = df_year_single['position_spotify'].iloc[-1]\n",
    "\n",
    "    # Save the results\n",
    "    spotify_top100_weeks.append((track_id, first_position, last_position, max_consecutive_weeks, starting_week_max, ending_week_max, starting_pos_max, ending_pos_max))\n",
    "\n",
    "# Sort the results by the number of consecutive weeks\n",
    "spotify_top100_weeks = sorted(spotify_top100_weeks, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print('Tracks with the highest number of consecutive weeks in the Spotify top 100:')\n",
    "#for track_id, first_position, last_position, max_consecutive_weeks, starting_week_max, ending_week_max, starting_pos_max, ending_pos_max in spotify_top100_weeks:\n",
    "#    if max_consecutive_weeks > 1:\n",
    "#        print(f'Track: {convert_track_id_to_name(track_id)} of id {track_id} - First Position: {first_position} - Last Position: {last_position} - Max Consecutive Weeks: {max_consecutive_weeks} - Starting Week: {starting_week_max} - Ending Week: {ending_week_max} - Starting Position: {starting_pos_max} - Ending Position: {ending_pos_max}')\n",
    "\n",
    "print('Tracks that did not start on the first week:')\n",
    "for track_id, first_position, last_position, max_consecutive_weeks, starting_week_max, ending_week_max, starting_pos_max, ending_pos_max in spotify_top100_weeks:\n",
    "    if starting_week_max != 487 and max_consecutive_weeks > 1:\n",
    "        print(f'Track: {convert_track_id_to_name(track_id)} of id {track_id} - First Position: {first_position} - Last Position: {last_position} - Max Consecutive Weeks: {max_consecutive_weeks} - Starting Week: {starting_week_max} - Ending Week: {ending_week_max} - Starting Position: {starting_pos_max} - Ending Position: {ending_pos_max}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
